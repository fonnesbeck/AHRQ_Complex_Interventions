{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right\" src=\"https://www.mc.vanderbilt.edu/documents/lmsa/images/VMS_Logo.png\">\n",
    "\n",
    "# Bayesian Meta-analysis Webinar\n",
    "\n",
    "### Christopher Fonnesbeck, Vanderbilt University School of Medicine\n",
    "\n",
    "#### 28 May 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The primary goal of this presentation is to give an overview on Bayesian inference, and in particular, on how to employ **probabilistic programming** to apply Bayesian methods to meta-analysis. Though this approach to statistical modeling has become more prevalent in the past two decades, and though it has several strengths that make it ideal for meta-analytic modeling, one of the largest obstacles for widespread adoption is the requirement for proficiency in computational methods in order to adopt them. The same off-the-shelf software that is used to conduct classical or frequentist analyses are generally not capable of running Bayesian analysis well.\n",
    "\n",
    "Hence, in the next hour I will provide a link between the *theory* of Bayesian statistics and the *practice* of building Bayesian models, using probabilistic programming. The outline is as follows:\n",
    "\n",
    "### 1. Introduction to Bayesian Statistical Analysis\n",
    "\n",
    "Incase you are unfamiliar with the Bayesian approach to statistical analysis, I will provide a very brief overview, with links to additional, more comprehensive reference material. This is intended to motivate the use of probabilistic programming tools for conducting meta-analysis.\n",
    "\n",
    "### 2. A Primer on Programming using Python\n",
    "\n",
    "Applying Bayesian methods to analyzing data effectively requires being able to write high-level software programs. While adding a level of complexity to the analysis procedure, this typically results in a more powerful avenue for analyzing data. The good news is there are several scientific programming languages available today that make coding and analysis much easier than with the previous generation of tools. We will learn the basics of these languages, Python, sufficiently to build and run a simple model.\n",
    "\n",
    "### 3. Using Probabilistic Programming to Construct Bayesian Models for Meta-analysis\n",
    "\n",
    "With theory in one hand and a powerful programming language in another, we will employ the probabilistic progrmaming paradigm to specify a meta-analysis. This will be done using a Python data analysis package called `PyMC`. We will step through the preparation of the data, encoding the model in software, then running the model and inspecting the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webinar materials\n",
    "\n",
    "This webinar is presented using an [IPython Notebook](http://ipython.org/notebook.html), an HTML-based, interactive programming environment for the Python programming language that allows text and other media to be integrated with code. I have placed the notebooks, which can be run interactively or viewed as a static web page, on a GitHub repository. \n",
    "\n",
    "It can be viewed or downloaded here:\n",
    "\n",
    "[https://github.com/fonnesbeck/AHRQ_Complex_Interventions](https://github.com/fonnesbeck/AHRQ_Complex_Interventions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Statistical Analysis\n",
    "\n",
    "Though many of you will have taken a statistics course or two during your undergraduate (or graduate) education, most of those who have will likely not have had a course in *Bayesian* statistics. Most introductory courses, particularly for non-statisticians, still do not cover Bayesian methods at all, except perhaps to derive Bayes' formula as a trivial rearrangement of the definition of conditional probability. Even today, Bayesian courses are typically tacked onto the curriculum, rather than being integrated into the program.\n",
    "\n",
    "In fact, Bayesian statistics is not just a particular method, or even a class of methods; it is an entirely different paradigm for doing statistical analysis.\n",
    "\n",
    "> Practical methods for making inferences from data using probability models for quantities we observe and about which we wish to learn. *-- Gelman et al. 2013*\n",
    "\n",
    "A Bayesian model is described by parameters, uncertainty in those parameters is described using probability distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All conclusions from Bayesian statistical procedures are stated in terms of *probability statements*\n",
    "\n",
    "![](images/prob_model.png)\n",
    "\n",
    "This confers several benefits to the analyst, including:\n",
    "\n",
    "- ease of interpretation, summarization of uncertainty\n",
    "- can incorporate uncertainty in parent parameters\n",
    "- easy to calculate summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian vs Frequentist Statistics: What's the difference?\n",
    "\n",
    "![can of worms](images/can-of-worms.jpg)\n",
    "\n",
    "Any statistical paradigm, Bayesian or otherwise, involves at least the following: \n",
    "\n",
    "1. Some **unknown quantities** about which we are interested in learning or testing. We call these *parameters*.\n",
    "2. Some **data** which have been observed, and hopefully contain information about the parameters.\n",
    "3. One or more **models** that relate the data to the parameters, and is the instrument that is used to learn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Frequentist World View\n",
    "\n",
    "![Fisher](images/fisher.png)\n",
    "\n",
    "- The data that have been observed are considered **random**, because they are realizations of random processes, and hence will vary each time one goes to observe the system.\n",
    "- Model parameters are considered **fixed**. The parameters' values are unknown, but they are fixed, and so we *condition* on them.\n",
    "\n",
    "In mathematical notation, this implies a (very) general model of the following form:\n",
    "\n",
    "<div style=\"font-size:35px\">\n",
    "\\\\[f(y | \\theta)\\\\]\n",
    "</div>\n",
    "\n",
    "Here, the model \\\\(f\\\\) accepts data values \\\\(y\\\\) as an argument, conditional on particular values of \\\\(\\theta\\\\).\n",
    "\n",
    "Frequentist inference typically involves deriving **estimators** for the unknown parameters. Estimators are formulae that return estimates for particular estimands, as a function of data. They are selected based on some chosen optimality criterion, such as *unbiasedness*, *variance minimization*, or *efficiency*.\n",
    "\n",
    "> For example, lets say that we have collected some data on the prevalence of autism spectrum disorder (ASD) in some defined population. Our sample includes \\\\(n\\\\) sampled children, \\\\(y\\\\) of them having been diagnosed with autism. A frequentist estimator of the prevalence \\\\(p\\\\) is:\n",
    "\n",
    "> <div style=\"font-size:25px\">\n",
    "> $$\\hat{p} = \\frac{y}{n}$$\n",
    "> </div>\n",
    "\n",
    "> Why this particular function? Because it can be shown to be unbiased and minimum-variance.\n",
    "\n",
    "It is important to note that new estimators need to be derived for every estimand that is introduced.\n",
    "\n",
    "### The Bayesian World View\n",
    "\n",
    "![Bayes](images/bayes.png)\n",
    "\n",
    "- Data are considered **fixed**. They used to be random, but once they were written into your lab notebook/spreadsheet/database they do not change.\n",
    "- Model parameters themselves may not be random, but Bayesians use probability distribtutions to describe their uncertainty in parameter values, and are therefore treated as **random**. In some cases, it is useful to consider parameters as having been sampled from probability distributions.\n",
    "\n",
    "This implies the following form:\n",
    "\n",
    "<div style=\"font-size:35px\">\n",
    "$$p(\\theta | y)$$\n",
    "</div>\n",
    "\n",
    "This formulation used to be referred to as ***inverse probability***, because it infers from observations to parameters, or from effects to causes.\n",
    "\n",
    "Bayesians do not seek new estimators for every estimation problem they encounter. There is only one estimator for Bayesian inference: **Bayes' Formula**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Inference, in 3 Easy Steps\n",
    "\n",
    "![123](images/123.png)\n",
    "\n",
    "Gelman et al. (2013) describe the process of conducting Bayesian statistical analysis in 3 steps.\n",
    "\n",
    "### Step 1: Specify a probability model\n",
    "\n",
    "As was noted above, Bayesian statistics involves using probability models to solve problems. So, the first task is to *completely specify* the model in terms of probability distributions. This includes everything: unknown parameters, data, covariates, missing data, predictions. All must be assigned some probability density.\n",
    "\n",
    "This step involves making choices.\n",
    "\n",
    "- what is the form of the sampling distribution of the data?\n",
    "- what form best describes our uncertainty in the unknown parameters?\n",
    "\n",
    "### Step 2: Calculate a posterior distribution\n",
    "\n",
    "The mathematical form $p(\\theta | y)$ that we associated with the Bayesian approach is referred to as a **posterior distribution**.\n",
    "\n",
    "> posterior /pos·ter·i·or/ (pos-tēr´e-er) later in time; subsequent.\n",
    "\n",
    "Why posterior? Because it tells us what we know about the unknown $\\theta$ *after* having observed $y$.\n",
    "\n",
    "This posterior distribution is formulated as a function of the probability model that was specified in Step 1. Usually, we can write it down but we cannot calculate it analytically. In fact, the difficulty inherent in calculating the posterior distribution for most models of interest is perhaps the major contributing factor for the lack of widespread adoption of Bayesian methods for data analysis. Various strategies for doing so comprise this tutorial.\n",
    "\n",
    "**But**, once the posterior distribution is calculated, you get a lot for free:\n",
    "\n",
    "- point estimates\n",
    "- credible intervals\n",
    "- quantiles\n",
    "- predictions\n",
    "\n",
    "### Step 3: Check your model\n",
    "\n",
    "Though frequently ignored in practice, it is critical that the model and its outputs be assessed before using the outputs for inference. Models are specified based on assumptions that are largely unverifiable, so the least we can do is examine the output in detail, relative to the specified model and the data that were used to fit the model.\n",
    "\n",
    "Specifically, we must ask:\n",
    "\n",
    "- does the model fit data?\n",
    "- are the conclusions reasonable?\n",
    "- are the outputs sensitive to changes in model structure?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why be Bayesian?\n",
    "\n",
    "At this point, it is worth addressing the question of why one might consider an alternative statistical paradigm to the classical/frequentist statistical approach. After all, it is not always easy to specify a full probabilistic model, nor to obtain output from the model once it is specified. So, why bother?\n",
    "\n",
    "> ... the Bayesian approach is attractive because it is useful. Its usefulness derives in large measure from its simplicity. Its simplicity allows the investigation of far more complex models than can be handled by the tools in the classical toolbox.  \n",
    "*-- Link and Barker 2010*\n",
    "\n",
    "We already noted that there is just one estimator in Bayesian inference, which lends to its ***simplicity***. Moreover, Bayes affords a conceptually simple way of coping with multiple parameters; the use of probabilistic models allows very complex models to be assembled in a modular fashion, by factoring a large joint model into the product of several conditional probabilities.\n",
    "\n",
    "Bayesian statistics is also attractive for its ***coherence***. All unknown quantities for a particular problem are treated as random variables, to be estimated in the same way. Existing knowledge is given precise mathematical expression, allowing it to be integrated with information from the study dataset, and there is formal mechanism for incorporating new information into an existing analysis.\n",
    "\n",
    "Finally, Bayesian statistics confers an advantage in the ***iterpretability*** of analytic outputs. Because models are expressed probabilistically, results can be interpreted probabilistically. Probabilities are easy for users (particularly non-technical users) to understand and apply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: confidence vs. credible intervals\n",
    "\n",
    "A commonly-used measure of uncertainty for a statistical point estimate in classical statistics is the ***confidence interval***. Most scientists were introduced to the confidence interval during their introductory statistics course(s) in college. Yet, a large number of users mis-interpret the confidence interval.\n",
    "\n",
    "Here is the mathematical definition of a 95% confidence interval for some unknown scalar quantity that we will here call \\\\(\\theta\\\\):\n",
    "\n",
    "<div style=\"font-size:25px\">\n",
    "$$Pr(a(Y) < \\theta < b(Y) | \\theta) = 0.95$$\n",
    "</div>\n",
    "\n",
    "how the endpoints of this interval are calculated varies according to the sampling distribution of $Y$, but for as an example, the confidence interval for the population mean when $Y$ is normally distributed is calculated by:\n",
    "\n",
    "$$Pr(\\bar{Y} - 1.96\\frac{\\sigma}{\\sqrt{n}}< \\theta < \\bar{Y} + 1.96\\frac{\\sigma}{\\sqrt{n}}) = 0.95$$\n",
    "\n",
    "It would be tempting to use this definition to conclude that there is a 95% chance \\\\(\\theta\\\\) is between \\\\(a(Y)\\\\) and \\\\(b(Y)\\\\), but that would be a mistake. \n",
    "\n",
    "Recall that for frequentists, unknown parameters are **fixed**, which means there is no probability associated with them being any value except what they are fixed to. Here, the interval itself, and not $\\theta$ is the random variable. The actual interval calculated from the data is just one possible realization of a random process, and it must be strictly interpreted only in relation to an infinite sequence of identical trials that might be (but never are) conducted in practice.\n",
    "\n",
    "A valid interpretation of the above would be:\n",
    "\n",
    "> If the experiment were repeated an infinite number of times, 95% of the calculated intervals would contain $\\theta$.\n",
    "\n",
    "This is what the statistical notion of \"confidence\" entails, and this sets it apart from probability intervals.\n",
    "\n",
    "Since they regard unknown parameters as random variables, Bayesians can and do use probability intervals to describe what is known about the value of an unknown quantity. These intervals are commonly known as ***credible intervals***.\n",
    "\n",
    "The definition of a 95% credible interval is:\n",
    "\n",
    "<div style=\"font-size:25px\">\n",
    "$$Pr(a(y) < \\theta < b(y) | Y=y) = 0.95$$\n",
    "</div>\n",
    "\n",
    "Notice that we condition here on the data $y$ instead of the unknown $\\theta$. Thus, the endpoints are fixed and the variable is random. \n",
    "\n",
    "We are allowed to interpret this interval as:\n",
    "\n",
    "> There is a 95% chance $\\theta$ is between $a$ and $b$.\n",
    "\n",
    "Hence, the credible interval is a statement of what we know about the value of $\\theta$ based on the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes' Formula\n",
    "\n",
    "The goal in Bayesian inference is to calculate the **posterior distribution** of our unknowns:\n",
    "\n",
    "<div style=\"font-size: 150%;\">  \n",
    "\\\\[Pr(\\theta|Y=y)\\\\]\n",
    "</div>\n",
    "\n",
    "This expression is a **conditional probability**. It is the probability of \\\\(\\theta\\\\) *given* the observed values of \\\\(Y=y\\\\).\n",
    "\n",
    "This posterior distribution is calculated using Bayes' formula:\n",
    "\n",
    "![bayes formula](images/bayes_formula.png)\n",
    "\n",
    "The equation expresses how our belief about the value of \\\\(\\theta\\\\), as expressed by the **prior distribution** \\\\(P(\\theta)\\\\) is reallocated following the observation of the data \\\\(y\\\\), as expressed by the posterior distribution the posterior distribution.\n",
    "\n",
    "The innocuous denominator \\\\(P(y)\\\\) cannot be calculated directly, and is actually the expression in the numerator, integrated over all \\\\(\\theta\\\\):\n",
    "\n",
    "<div style=\"font-size: 150%;\">  \n",
    "\\\\[Pr(\\theta|y) = \\frac{Pr(y|\\theta)Pr(\\theta)}{\\int Pr(y|\\theta)Pr(\\theta) d\\theta}\\\\]\n",
    "</div>\n",
    "\n",
    "The intractability of this integral is one of the factors that has contributed to the under-utilization of Bayesian methods by statisticians.\n",
    "\n",
    "### Priors\n",
    "\n",
    "Once considered a controversial aspect of Bayesian analysis, the prior distribution characterizes what is known about an unknown quantity before observing the data from the present study. Thus, it represents the information state of that parameter. It can be used to reflect the information obtained in previous studies, to constrain the parameter to plausible values, or to represent the population of possible parameter values, of which the current study's parameter value can be considered a sample.\n",
    "\n",
    "### Likelihood functions\n",
    "\n",
    "The likelihood represents the information in the observed data, and is used to update prior distributions to posterior distributions. This updating of belief is justified becuase of the **likelihood principle**, which states:\n",
    "\n",
    "> Following observation of \\\\(y\\\\), the likelihood \\\\(L(\\theta|y)\\\\) contains all experimental information from \\\\(y\\\\) about the unknown \\\\(\\theta\\\\).\n",
    "\n",
    "Bayesian analysis satisfies the likelihood principle because the posterior distribution's dependence on the data is only through the likelihood. In comparison, most frequentist inference procedures violate the likelihood principle, because inference will depend on the design of the trial or experiment.\n",
    "\n",
    "## Bayesian Updating\n",
    "\n",
    "Bayes formula is used to update the prior distribution to a posterior distribution, using the information in the likelihood. One of the simplest examples is estimating the probability of getting heads on a coin, based on repeated samples. This updating is shown graphically below.\n",
    "\n",
    "![Bayesian updating](http://d.pr/i/1by1M+)\n",
    "*(via [Probabilistic Programming and Bayesian Methods for Hackers](http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/))*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Programming\n",
    "\n",
    "## What is Python?\n",
    "\n",
    "Python is:\n",
    "\n",
    "* a modern, general-purpose programming language\n",
    "* open-source \n",
    "* an *interpreted* language \n",
    "\n",
    "It is often compared to languages like R and Ruby. It offers the power and flexibility of lower level (*i.e.* compiled) languages, without the steep learning curve, and without most of the associated  pitfalls for new or non-expert users. The language is very clean and readable, and it is available for almost every modern computing platform.\n",
    "\n",
    "![python](http://imgs.xkcd.com/comics/python.png)\n",
    "\n",
    "*(via [xkcd](http://imgs.xkcd.com/comics/python.png))*\n",
    "\n",
    "## Why use Python for scientific programming?\n",
    "\n",
    "Python offers a number of advantages to scientists, both for experienced and novice programmers alike:\n",
    "\n",
    "***Powerful and easy to use***  \n",
    "\n",
    "* avoids the power/ease-of-use tradeoff\n",
    "* clean, readable syntax\n",
    "* rich standard library\n",
    "\n",
    "***Interactive***  \n",
    "\n",
    "* run interactively from the command line\n",
    "* run non-interactiely with scripts\n",
    "* IPython notebooks for reproducibility\n",
    "\n",
    "\n",
    "***Extensible***  \n",
    "\n",
    "* useful in mixed-language environments\n",
    "* advanced users can write fast extensions using Fortran, C, or Cython\n",
    "\n",
    "\n",
    "***Third-party modules***  \n",
    "\n",
    "There is a vast body of Python modules created outside the auspices of the Python Software Foundation. These include utilities for database connectivity, mathematics, statistics, and charting/plotting. Some notables include:\n",
    "\n",
    "* `NumPy`: array data structures and array operations.\n",
    "* `SciPy`: set of high level science and engineering modules, including optimization, integration, special functions, signal and image processing, genetic algorithms, ODE solvers, and others.\n",
    "* `Matplotlib`: 2D plotting library for publication-quality figures.\n",
    "* `Pandas`: high-performance, easy-to-use tabular data structures and data analysis tools. In particular, the `DataFrame` class is useful for spreadsheet-like representation and mannipulation of data. Also includes high-level plotting functionality.\n",
    "* `IPython`: enhanced Python shell, designed to increase the efficiency and usability of coding, testing and debugging.\n",
    "\n",
    "***Free and open***  \n",
    "\n",
    "Python is released on all platforms under the GNU public license, meaning that the language and its source is freely distributable. Not only does this keep costs down for scientists and universities operating  under a limited budget, but it also frees programmers from licensing concerns for any software they may develop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Let's look at some Python code, beginning with the simplest possible operation: assignment of values to variables. Let's create a variable called `weight_kg` (its nice to give your variables meaningful names!) and assign it a particular value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_kg = 79.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we have done here is created a \"label\" and bound a particular value to it. We can take a look at the current value of `weight_kg`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79.5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_kg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to know what kind of variable this is, the `type` function will give us this information. We **call** a function by following the function name with parentheses that enclose any relevant **arguments** the function may need to operate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(weight_kg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also use the `print` function to report the value of `weight_kg` in a meaningful way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My weight in kilograms is 79.5\n"
     ]
    }
   ],
   "source": [
    "print('My weight in kilograms is', weight_kg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might now use an arithmetic operator on our variable to create a new one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174.9"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_lbs = weight_kg * 2.2\n",
    "\n",
    "weight_lbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables in Python are **dynamically typed**, which means that unlike many languages you do not have to pre-specify what kind of variable you need; it automatically figures out the type for you. Moreover, if you want to use the variable to hold another kind of variable, you can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'none of your business!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_kg = 'none of your business!'\n",
    "\n",
    "weight_kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(weight_kg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use Python for data analysis, we need a more general data structure, since we will be dealing with entire datasets, and we do not wish to assign each value to individual variables. The simplest such *vector-valued* data structure is the Python **list**, which is specified by surrounding comma-separated value with square braces. \n",
    "\n",
    "Let's look at a list of data, which we will use later in an example analysis. These are the occurrences of deaths during cardiac surgery procedures at 12 different hospitals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deaths = [2, 18, 8, 46, 8, 13, 9, 31, 14, 8, 29, 24]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may want to know how many items are in this list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(deaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may wish to look at the first item of the list only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deaths[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The square brackets are used to pass an index (or set of indices) to the list. Notice that the first item is indexed by zero, rather than by one. \n",
    "\n",
    "Or, we may wish to look at the first 4 values of the list only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 18, 8, 46]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deaths[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The colon specifies a sqeuence from a to b by `a:b`. If `a` is omitted, it assumes to begin at the start of the list, and if `b` is omitted, it assumes to end at the end of the list. In fact, one way to generate a copy of the list is to pass it a colon, all by itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 18, 8, 46, 8, 13, 9, 31, 14, 8, 29, 24]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deaths[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use indices to **replace** values in a list, as well as just to look at them. For example, there is an error in this dataset: the number of deaths at the first (index zero!) hospital should be zero rather than two. We can fix this by assigning zero to the indexed element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deaths[0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may wish to perform an operation on each of the elements of the data list. The simplest way to do this is to construct a **loop** and execute the operation on each element, in turn. We can do this using a `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "324\n",
      "64\n",
      "2116\n",
      "64\n",
      "169\n",
      "81\n",
      "961\n",
      "196\n",
      "64\n",
      "841\n",
      "576\n"
     ]
    }
   ],
   "source": [
    "for x in deaths:\n",
    "    x2 = x**2\n",
    "    print(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `for` loop above assigns each of the values in `deaths` to the variable `x` in order from beginning to end. The loop includes any statements after the colon that are indented; there is no need for brackets, as you see in most other programming languages, making the code easy to read.  So, for each element, the value is squared (using the `**` power operator) and assigned to a new variable `x2`, then this value is printed before moving on to the next item in the list.\n",
    "\n",
    "Notice a couple of things here: we did not have to use explicit indexes, counting from zero to eleven, as you might have expected, to obtain each of the values in `deaths` (though we could have!). This is because Python lists are **iterable**, which means control structures like `for` loops know how to extract each element from it, as needed.\n",
    "\n",
    "If we wanted to save these values to a new list, we would have to initialize an empty list and populate it using the `for` loop, using the `append` method for lists. \n",
    "\n",
    "> A **method** is just a function that is associated with a particular Python object\n",
    "\n",
    "Here is what that looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 324, 64, 2116, 64, 169, 81, 961, 196, 64, 841, 576]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize an empty list\n",
    "deaths_squared = []\n",
    "\n",
    "for x in deaths:\n",
    "    deaths_squared.append(x**2)\n",
    "    \n",
    "deaths_squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider what we would have to do in order to calculate the *rate* of mortality for the procedure, by dividing each death count by the number of surgeries performed in each hospital ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "surgeries = [47, 148, 119, 810, 211, 196, 148, 215, 207, 97, 256, 360]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somehow we would have to iterate, pair-wise, over both lists and divide one element by the other. We could do this with the help of a useful Python function called `zip`, which iterates over two iterables, pair-wise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.12162162162162163,\n",
       " 0.06722689075630252,\n",
       " 0.056790123456790124,\n",
       " 0.037914691943127965,\n",
       " 0.0663265306122449,\n",
       " 0.060810810810810814,\n",
       " 0.14418604651162792,\n",
       " 0.06763285024154589,\n",
       " 0.08247422680412371,\n",
       " 0.11328125,\n",
       " 0.06666666666666667]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "death_rate = []\n",
    "\n",
    "for x,n in zip(deaths, surgeries):\n",
    "    death_rate.append(x/n)\n",
    "    \n",
    "death_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more advanced manipuation of vector-valued variables, it is recommended that we move away from Python's built-in lists to the more capable `ndarray` structure that is provided by the Numpy package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 18,  8, 46,  8, 13,  9, 31, 14,  8, 29, 24])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "deaths_array = numpy.array(deaths)\n",
    "deaths_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surgeries_array = numpy.array(surgeries)\n",
    "type(surgeries_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the many useful features of the Numpy array is that it performs element-wise operations, avoiding the need to explicitly loop over its elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,  324,   64, 2116,   64,  169,   81,  961,  196,   64,  841,\n",
       "        576])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deaths_array**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.12162162,  0.06722689,  0.05679012,  0.03791469,\n",
       "        0.06632653,  0.06081081,  0.14418605,  0.06763285,  0.08247423,\n",
       "        0.11328125,  0.06666667])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deaths_array/surgeries_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `matplotlib` package can be used to plot our data. For example, we can make a bar chart of the surgeries at each hospital, and overlay the number of deaths in a different color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Container object of 12 artists>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": [
       "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEBCAYAAABysL6vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\n",
       "AAALEgAACxIB0t1+/AAAEBpJREFUeJzt3W+IZXd9x/H3Z/yTKOm6dmtjCIVVZ6sPShGkxD/RmJ1U\n",
       "UGyhCFpK8U8FdbVGWqxKG2hmCAHbutU2dAUpFR9oq0WRQsju7M7a3UhIwQfNg+KSa7s0hK6FRScp\n",
       "yaaV/fbBubHT2Ttz7+6eO2fzm/cLJNlvDmd/w7rv+c25956TqkKS1J6FoRcgSZoPAy9JjTLwktQo\n",
       "Ay9JjTLwktQoAy9JjZoa+CQfSvJQkmNJFsezpSSnk5xKcnDDsRPnkqSdl+3eB5/kRcDJqnp9kn3A\n",
       "F4F3Aw8CS0CAo1X1liSZNJ/3FyBJmuz5U/57gOcneSGwDtwIHADOVNUFgCSj8c5+YdK8qkbzW74k\n",
       "aSvbBr6qnkpyL/AA8CTwUrrIryc5TPcNYB3YRxf4SXMDL0kDmLaDp6q+CXwTIMn3gHPAXuAQXciP\n",
       "AOfpAj9pLkkawNTAPyvJO4B/Bn5Ad5kGupAvVtUoycKk+VbnO3HihDfBkaQrsLS0lFmOmxr4JH8N\n",
       "vBr4L+C3q+pikmXgOFDAMsBW8z4WKUnqXM7meJZLNB+cMFsFVmedS5J2nh90kqRGGXhJapSBl6RG\n",
       "GXhJapSBl6RGGXhJapSBl6RGGXhJapSBl6RGGXhJapSBl6RGGXhJapSBl6RGGXhJapSBl6RGGXhJ\n",
       "apSBl6RGTQ18kg8keTjJg0luH8/uSHI6yakkBzccuzRpLknaebM8dPv3gNcCNwAPJHkT3fNWl+ge\n",
       "rn0UWEsSYGXzfB6LliRNN0vgHwHuAH6eLtoHgDNVdQEgySjJIt1PA5fMq2o0n6Vro4cfffzL5554\n",
       "Zn/f5335nuvO3nLg5vf3fV5J8zdL4E8D76ML+NeAfcB6ksN0O/X18Wxhi7mB3wHnnnhm/z1rZ2/r\n",
       "+7x3Hdzf9ykl7ZBtA5/kVcDBqnrP+NcngTuBvcAhupAfAc7TBX7SXJI0gGk7+AXgJQBJXkAX8BHd\n",
       "ZRroQr5YVaMkC5Pm/S9ZkjSLbQNfVY+O3xHzEF20P19VTydZAY4DRfeCK1V1Mcny5rkkaRhTr8FX\n",
       "1b3AvZtmx4BjE45dBVZ7W50k6Yr5QSdJapSBl6RGGXhJapSBl6RGGXhJapSBl6RGGXhJapSBl6RG\n",
       "GXhJapSBl6RGGXhJapSBl6RGGXhJapSBl6RGGXhJapSBl6RGGXhJatS2gU+yJ8nJJGvjf/54PL8j\n",
       "yenx4/wObjh+adJckrTzpj2T9QngdoAkvwx8PEnonre6RPec1qPA2ni+snk+v6VLkrYz9ZmsG3wc\n",
       "+EvgAHCmqi4AJBklWaT7aeCSeVWN+l60JGm6mQKf5GeBX6iqR5K8AVhPcphup74O7KML/KS5gZek\n",
       "Acy6g/8Q8KXxv58H9gKH6EJ+ZDxb2GIuSRrA1MAneR7wa8Cbx6MR3WUa6EK+WFWjJAuT5j2vV5I0\n",
       "o1l28L8B/ENVXQSoqotJloHjQNG94LrlXJI0jKmBr6q/nzBbBVZnnUuSdp4fdJKkRhl4SWqUgZek\n",
       "Rhl4SWqUgZekRhl4SWqUgZekRhl4SWqUgZekRhl4SWqUgZekRhl4SWqUgZekRhl4SWqUgZekRhl4\n",
       "SWqUgZekRk0NfJKbk6wlOZXkc+PZHUlOj2cHNxy7NGkuSdp5szyT9c+AP6qqhwCShO55q0t0D9c+\n",
       "CqyN5yub5/NYtCRpum0Dn2QBWHw27mMHgDNVdWF8zCjJIt1PA5fMq2o0p7VLkrYxbQf/MuD6JN8C\n",
       "9gD3AeeA9SSH6Xbq68A+usBPmht4SRrAtMCfB34MvGt87HeB3wH2AofoQn5kfNzCFnNJ0gC2DXxV\n",
       "/STJY8BNVfV4kgt0O/ID40NCdwlnNL6cc8l8XguXJG1vlhdZPwN8Kcke4OtV9XSSFeA4UHQvuFJV\n",
       "F5Msb55LkoYxNfBV9e/AOzbNjgHHJhy7Cqz2tjpJ0hXzg06S1CgDL0mNMvCS1CgDL0mNMvCS1CgD\n",
       "L0mNMvCS1CgDL0mNMvCS1CgDL0mNMvCS1CgDL0mNMvCS1CgDL0mNMvCS1CgDL0mNmhr4JH+T5KEk\n",
       "a0neO57dkeR0klNJDm44dmnSXJK082Z5ZF8B766qxwCShO5xfEt0z149CqyN5yub5/NYtCRpulkC\n",
       "H/7/Tv8AcKaqLgAkGSVZHB9zydwHb0vSMGYJ/JPAV5OcB34f2AesJzlMF//18Wxhi7mBl6QBzPLQ\n",
       "7TsBkrwW+FPgU8Be4BBdyI8A5+kCP2kuSRrALDv4Z10A/gf4Ad1lGuhCvlhVoyQLk+a9rVSSdFmm\n",
       "Bj7J3wI30V2q+VhVXUyyDBynewF2GWCruSRpGLNcovnNCbNVYHXWuSRp5/lBJ0lqlIGXpEYZeElq\n",
       "lIGXpEYZeElqlIGXpEYZeElqlIGXpEYZeElqlIGXpEYZeElqlIGXpEYZeElqlIGXpEYZeElqlIGX\n",
       "pEYZeElq1EyBT/LCJGeTfHT86zuSnE5yKsnBDcctTZpLknberA/d/gjwPYAkoXve6hLdw7WPAmvj\n",
       "+crmed8LliTNZuoOPsmLgLcB3x6PDgBnqupCVT0NjJIsbjOXJA1glh38ncB9wI3jX+8D1pMcptup\n",
       "r49nC1vMR30vWpI03bY7+CR7gDdX1QPPjoDzwF7gD8f/e+l4ttVckjSAaTv4W4HrknwVeCXwPOA0\n",
       "3eUY6IK/WFWjJAuT5nNYsyRpBtsGvqruB+4HSPJe4IaqeiTJCnAcKLoXXKmqi0mWN88lScOY9V00\n",
       "VNVXNvz7MeDYhGNWgdV+liZJuhp+0EmSGmXgJalRBl6SGmXgJalRBl6SGmXgJalRBl6SGjXz++Al\n",
       "SZd6+NHHv3zuiWf2933el++57uwtB25+/9Wcw8BL0lU498Qz++9ZO3tb3+e96+D+qz6Hl2gkqVEG\n",
       "XpIaZeAlqVEGXpIaZeAlqVEGXpIaZeAlqVEGXpIaNTXwSe5JspZkNckrxrOlJKeTnEpycMOxE+eS\n",
       "pJ039ZOsVXUXQJI3AZ9OcghYAZboHq59FFhLkknzOa1bkjTF5dyq4Bbg+8AB4ExVXQBIMkqySPfT\n",
       "wCXzqhr1vWhJ0nQzBT7JPwI3AbcCrwLWkxym26mvA/voAj9pbuAlaQAzBb6qbkvyK8BXgI8De4FD\n",
       "dCE/ApynC/ykuSRpAJdzieaHQAE/oLtMA13IF6tqlGRh0ry3lUqSLsvUwCf5O+DngKeB362qi0mW\n",
       "geN0wV8G2GouSRrGLO+iec+E2SqwOutckrTz/KCTJDXKwEtSowy8JDXKwEtSowy8JDXKwEtSowy8\n",
       "JDXqcj7JKknXvIcfffzL5554Zn/f5335nuvO3nLg5vf3fd55MvCSmnLuiWf237N29ra+z3vXwf19\n",
       "n3LuvEQjSY0y8JLUKAMvSY0y8JLUKAMvSY0y8JLUKAMvSY0y8JLUqFke2fdF4NV0z1n9QFX9W5Il\n",
       "4G66R/PdXVVr42MnziVtzU9eal5meWTfRwCS3A78QZKPASvAEl30jwJrSTJpPqd1a0DzChLszij5\n",
       "yUvNy+XcquBJ4L+BA8CZqroAkGSUZJHucs8l86oa9b1oDWteQQKjtBP8iWH3uJzAfxD4ArAPWE9y\n",
       "mG6nvj6eLWwxN/DSNcSfGHaPmQKf5J10u/PvJ/lFYC9wiC7kR4DzdIGfNJckDWCWF1lfB7y1qj45\n",
       "Ho3oLtNAF/LFqholWZg073vB2n285i9dmVl28N8AHktyEnikqj6RZAU4TvdumWWAqrqYZHnzXLpa\n",
       "XvOXrsws76J55YTZMeDYhPkqsNrP0iRJV8MPOklSowy8JDXKwEtSowy8JDXKwEtSowy8JDXKwEtS\n",
       "owy8JDXKwEtSoy7nbpLPad4iVbPy3jdqxa4JvLdI1ay8941a4SUaSWqUgZekRhl4SWqUgZekRhl4\n",
       "SWrUrnkXzU7zbZmShjbLM1lvBQ4D36mqT41nS8DddI/mu7uq1rab70a+LVPS0GbZwV8H3Au8ESBJ\n",
       "gBVgie7h2keBta3mc1izJGkGU6/BV9UJ4EcbRgeAM1V1oaqeBkZJFreZS5IGcCXX4PcB60kO0+3U\n",
       "18ezhS3mo57WKkm6DFcS+PPAXuAQXciPjGcLW8wlSQO4nMBn/M8R3eWYZ2eLVTVKsjBp3s8yJT1X\n",
       "+Y6y4czyLppPA28Hbkyyp6o+nGQFOE73bpllgKq6mGR581zS7uY7yoYzNfBV9Vngs5tmx4BjE45d\n",
       "BVZ7W50k6Yr5SVZJapSBl6RGGXhJapSBl6RGGXhJapSBl6RGGXhJapSBl6RGGXhJatSgT3T69vf+\n",
       "9Tt9n3O33p/il576z9f8+Ut+2Pt5b3rqxa+BV/Z+XknzN2jgvT9Ff178+GPX3/rh3+r9vP/xta9f\n",
       "D6/v/byS5s9LNJLUKAMvSY0a9BKNJF8/0fwYeGlgvn7SL79h/h8DL6kpO/0N81r+htJ74JMsAXfT\n",
       "PdXp7qpa6/v3uBLX8h+CpOeua/knsF4DnyTACrBE91zWo8A1Efhr+Q9B2kludnaPvnfwB4AzVXUB\n",
       "IMkoiQ/f1lWZV5Bgd0bJzc7u0Xfg9wHrSQ7T7eDXx7NdF/iWd0k7Hdx5BQkmR8lvKP1q+e/Cta7v\n",
       "wJ8H9gKH6AJ/ZDzbdVreJe10cHda61/fTmv578K1LlXV38mSBeAUcAfdh6iOVdWtk449ceJEf7+x\n",
       "JO0iS0tLmeW4XgMPkORXgT+mexfNSlWt9vobSJJm0nvgJUnXBu9FI0mNMvCS1CgDL0mNGiTwSZaS\n",
       "nE5yKsnBIdYwL0m+mORkku8kecXQ65mHJC9McjbJR4deS9+S3Jxkbfz/zc8NvZ6+JflAkoeTPJjk\n",
       "9qHXc7WS3Jrkn5L8yYZZM33Z4uubuTE7/iLr+HYGD7LhdgZV9ZYdXcQOGP/leXdVHRp6LX1Lcidw\n",
       "G3Ciqv5q6PX0KcnXgL+oqoeGXss8JHkEeC1wA93fvTcMvKSrMr731c8Ab6yqT7XWl81f36b/NrUx\n",
       "Q+zgf3o7g6p6GhglWRxgHfP2JPDM0IvoW5IXAW8Dvj30Wvo2/hzHYqtxH3uE7nMqvw48MPBarlpV\n",
       "nQB+tGHUVF8mfH0bTW3MELcL3i23M/gg8IWhFzEHdwL3ATcOvZA5eBlwfZJvAXuA+6rqWwOvqW+n\n",
       "gffRbe6+NvBa5mG39AVmaMwQgW/+dgZJ3km3i/j+0GvpU5I9wJur6rNJ3kf359eS88CPgXfR/d34\n",
       "bpIHxjvB57wkrwIOVtV7xr8+meR4VT018NL61HxfYPbGDBH4Ed2PUdD9ATR1t8kkrwPeWlWfHHot\n",
       "c3ArcF2Sr9Ld5el5SU5W1b8MvK5eVNVPkjwG3FRVjye5MPSaerYAvAQgyQvoQnhx0BX159nNRqt9\n",
       "+elm6nIas+OBr6qLSZaB43S3M1je6TXM2TeAx5KcBB6pqk8MvaC+VNX9wP0ASd4L3NBK3Df4DPCl\n",
       "8U8r32hl9w5QVY+O31nyEF0wPv/srb2fq5J8Gng7cGOSPVX14SQrNNKXSV8fl9EYb1UgSY3yg06S\n",
       "1CgDL0mNMvCS1CgDL0mNMvCS1CgDL0mNMvCS1CgDL0mN+l+/KJOIV/uKdwAAAABJRU5ErkJggg==\n"
      ],
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10819e978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "plt.bar(range(12), surgeries)\n",
    "plt.bar(range(12), deaths, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a hands-on, in-depth introduction to Python programming I suggest following the materials on the [Software Carpentry](http://swcarpentry.github.io/python-novice-inflammation/) website, and registering for one of their [workshops](http://software-carpentry.org/workshops/index.html) if you are interested in becoming proficient with scientific computing in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Programming with PyMC\n",
    "\n",
    "Beyond the basics of Python programming, our goal here is to expose you to **probabilistic programming**, and how it can be used to build Bayesian statistical models.\n",
    "\n",
    "![PPL](http://d.pr/i/16Rz5+)\n",
    "\n",
    "The practical difference between probabilistic programming and standard computer programming is that rather than being able to assign simple numeric values (or other data types) to variables, probabilistic programming allows entire probability distributions to be assigned to variables, and manipulated.\n",
    "\n",
    "***Why is this relevant?*** In Bayesian inference, recall that we are compelled to specify a *full probability model* that assigns probability distributions to all unknown quantities, in a way that represents our current belief in the possible values that they may take. How do we represent this in computer code? Using a probabilistic programming language, we can simply assign a distribution to a variable.\n",
    "\n",
    "For example, consider the mortality rate for infant cardiac surgeries above; since it is a value constrained to be between zero and one, we might use a **beta distribution**.\n",
    "\n",
    "![beta](http://d.pr/i/19sPw+)\n",
    "\n",
    "For example, one choice might be:\n",
    "\n",
    "\\\\[\\text{rate} \\sim \\text{Beta}(1, 1)\\\\]\n",
    "\n",
    "This parameterization assigns equal probability to all possible values on the unit interval. Here are some sample values from a `Beta(1,1)` distribution using Numpy's `random` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples = numpy.random.beta(1, 1, size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Incidentally, this is the distribution that was used as the prior for the coin-tossing example that was shown in the first section.)\n",
    "\n",
    "We can easily look at the distribution of the resulting samples, using `hist` function from `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a probabilistic programming framework, we would be able to assign a beta distribution to a particular parameter, and combine it with other parameters and data in order to provide inference.\n",
    "\n",
    "Python is not by itself a probabilistic language, but the PyMC package provides this functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymc import Beta\n",
    "\n",
    "rate = Beta('rate', alpha=1, beta=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyMC includes classes of objects that represent most of the common statistical distributions. We can create variables representing these probabilities by passing it a name and the appropriate parameters.\n",
    "\n",
    "The `rate` object above is now an *instance* of a stochastic random variable that follows a beta distribution. It is given a value (drawn randomly if none is specified):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rate.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sample new values from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rate.random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain the log-probability of the current value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rate.logp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most importantly, `rate` can be combined with other stochastic variables to specify a particular model. All PyMC objects will have **parents** that partly determine its value, in this case these are the beta parameters `alpha` and `beta` that were both assigned values of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rate.parents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `parents` are stored in another standard Python data structure, called a `dictionary`. Dictionaries are sets of key-value pairs, for which the keys can be used to retrieve their corresponding values. So, if we want the value for a particular parent, we just index it by name (rather than by an array index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rate.parents['beta']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables sometimes have **children** that, in turn, depend on their values. We have not specified any such relationship for `rate`, so it is just an empty set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rate.children"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `set` is yet another data structure that is like a dictionary, but consists only of values, and no corresponding keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we build a model with these components? Continuing with this simple example, we may want to use our `rate` object as a prior for an unknown rate that we wish to estimate. For example, we may want to estimate the mortality rate in the first hospital in our dataset.\n",
    "\n",
    "There were zero deaths in 47 operations. A natural model for this type of **bounded count** data is a binomial distribution. So, we can use the `Binomial` class in PyMC to model the data from one hostpital. \n",
    "\n",
    "Let's use the `help` function to get the documentation for `Binomial`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymc import Binomial\n",
    "\n",
    "help(Binomial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This provides a lot of information about this stochastic. You can see that there is a list of arguments, some which are associated with assigned values, and others that do not have assigned values. The former are called *keyword* arguments, and are optional; if values are not provided by the user, then the values shown in the documentation are passed as *default* values. You can see here that only three variables are required: `name`, `n` (the sample size), and `p` (the probability of an event).\n",
    "\n",
    "We will use 47 for the sample size, `rate` as the probability (its true value is unknown), and 0 for the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = Binomial('d', n=47, p=rate, value=0, observed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we inclued a fifth argument, `oberved=True`. This is used to recognize that the particular value of this variable was observed (*i.e.* it comes from our dataset) and should not be changed. As with `rate`, which is an unobserved variable, we can sample from `d` but the value does not change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now inspect some of the attributes of our variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rate.children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d.parents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our very simple model, `rate` is the prior distribution of the unknown variable, and `d` is the likelihood that provides the information from the data.\n",
    "\n",
    "We can use one of several statistical methods to estimate the true value of `rate`. The simplest of these is the **maximum a posteriori (MAP)** estimate for the posterior. This simply uses optimization to seek out the value of `rate` that maximizes its posterior distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymc import MAP\n",
    "\n",
    "model = MAP([rate, d])\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have another PyMC object `MAP` that retains a method for fitting the unknown variables using MAP. We can initiate this using the `fit()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When `fit` has completed, the value of `rate` will now be the MAP estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rate.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we might expect, the estimate is very close to zero!\n",
    "\n",
    "(N.B. this turns out not to be a very good estimate--the true posterior is closer to 0.01--because the MAP approximation is not appropriate for this model, but this was just a heuristic example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Meta-analysis using PyMC\n",
    "\n",
    "Let's finish with a simple, but non-trivial example: performing a meta-analysis on the mortality risk of cardiac surgery data. The empirical estimates from each hospital vary from a rate of zero to over 0.14. Because of the heterogeneity across sites, it would likely be a mistake to simply pool the data in order to obtain a meta-estimate (Bayesian or otherwise). Similarly, giving up and declaring the outcomes to be too heterogeneous to combine is unhelpful. Using a hierarchical meta-analysis, we can attempt to estimate both the expected mortality rate, and the *variation* in this rate across sites.\n",
    "\n",
    "As we will see, this approach will allow us to estimate the expected mortality both within the sites that were included in the meta-analysis, as well as for any site that was not included in the meta-analysis, making it a useful tool for prediction.\n",
    "\n",
    "Let's construct the meta-analytic model by following the \"3 Steps\" outlined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Specify Probability Model\n",
    "\n",
    "For the meta-analysis, we will also use a binomial likelihood, since it is the same sort of data. Here, however, each hospital will have its own probability of mortality.\n",
    "\n",
    "$$d_i \\sim \\text{Binomial}(n_i, p_i)$$\n",
    "\n",
    "Where this model differs is that rather than simply placing a prior on each $p_i$, we want to *model* them hierarchically. That is, we believe that the probability of a death event varies over the hospitals according to some model.\n",
    "\n",
    "$$p_i \\sim P$$\n",
    "\n",
    "One choice for the model $P$ is the beta distribution, but we will instead specify a logit-normal model:\n",
    "\n",
    "$$\\text{logit}(p_i) = \\log\\left[\\frac{p_i}{1-p_i}\\right] \\sim N(\\mu, \\tau)$$\n",
    "\n",
    "This allows us to model the mean and variance (or here, the inverse-variance (precision) $\\tau$) rather than the more abstract scale and shape parameters of the beta distribution.\n",
    "\n",
    "Finally, we require priors on the unknown **hyperparameters** for the normal distribution, $\\mu$ and $\\tau$; we will assign normal and exponential priors, respectively.\n",
    "\n",
    "Here are the priors in PyMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymc import Normal, Exponential\n",
    "\n",
    "mu = Normal('mu', 0, 0.001, value=0)\n",
    "tau = Exponential('tau', 0.01, value=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These priors look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(numpy.random.normal(0, 1000, size=10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(numpy.random.exponential(100, size=10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They will provide adequate prior probability to any plausible values for the probability of mortality.\n",
    "\n",
    "Thus, the logit-scale probabilities are modeled in PyMC as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit_p = Normal('logit_p', mu, tau, size=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are modeling 12 probabilities, we can specify the additional argument `size=12`, which will draw 12 random samples from a normal distribution with mean `mu` and precision `tau`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logit_p.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can use these in a binomial probability function, however, we must inverse-logit transform them into probabilities. This transformation is:\n",
    "\n",
    "$$\\text{logit}^{-1}(x) = \\frac{1}{1 + \\exp(-x)}$$\n",
    "\n",
    "This requires two components in PyMC: (1) an implementation of the inverse-logit function and (2) A **deterministic** variable that applies this function to the `logit_p` variable we defined above.\n",
    "\n",
    "In Python, functions are defined using the `def` command, followed by the name of the function and its arguments. Anything inside the code block below is executed when the function is called, until some value is returned. Here is the inverse-logit function in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def invlogit(x):\n",
    "    return 1/(1 + numpy.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "invlogit(-1.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To incorporate this function into our model, we need to turn it into another type of unboserved variable: a **deterministic** variable. These are nodes in the model that are *completely* determined by their parents. \n",
    "\n",
    "PyMC is very flexible in that one can take any function and convert it into a deterministic variable using something called a **decorator**. A decorator is a special function that wraps another function to endow it with additional attributes or functionality (hence, \"decorates\" it). The decorator in this case is called `deterministic` and is specified using the `@` symbol. \n",
    "\n",
    "Our probabilities, appropriately transformed, now look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymc import deterministic\n",
    "\n",
    "@deterministic\n",
    "def p(x=logit_p):\n",
    "    return 1/(1 + numpy.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we code the likelihood, which we have already seen above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = Binomial('d', n=surgeries, p=p, value=deaths, observed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the directed acyclic graph (DAG) of the full model:\n",
    "\n",
    "![DAG](http://d.pr/i/1aRVL+)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Calculate Posterior Distribution of Parameters\n",
    "\n",
    "This meta-analytic model, though simple, is not easily estimated analytically, and this is the case for most non-trivial Bayesian models. One may use approximation methods, as we did with the previous example, but the *de facto* standard today is the use of Markov chain Monte Carlo (MCMC) methods. \n",
    "\n",
    "MCMC draws serially-dependent samples using an algorithm that is guaranteed to converge to the true posterior distribution, such that the corresponding draws, once converged, will be identical to draws from the true posterior distribution of the parameters. So, the output from MCMC runs is an entire *distribution* of values for each unknown variable, which we can use to calculate the mean, median, variance, quantiles, or other statistics of interest.\n",
    "\n",
    "To use MCMC sampling in PyMC, we use an `MCMC` object, to which we pass all the variables in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymc import MCMC\n",
    "\n",
    "variables = [mu, tau, logit_p, p, d]\n",
    "meta_analysis = MCMC(variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an MCMC model, we can sample from it. In this example, we will draw 20,000 samples. However, we will discard the first 10,000 of them. We do not want to use samples from the MCMC algorithm until it has converged; to be reasonably confident that convergence has occurred, we throw away a conservative \"burn-in\" interval from the beginning of the sample, and use the remaining sample for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meta_analysis.sample(20000, burn=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what a sample from a MCMC algorithm looks like, in this case, for the hierarchical mean $\\mu$. The plot contains 3 panels:\n",
    "\n",
    "1. **Trace**: the upper left panel is the sequence of draws from the estimated posterior for $\\mu$. It illustrates how the simulation progressed for that variable; any patterns (increasing or decreasing mean or variance) may suggest non-convergence.\n",
    "\n",
    "2. **Histogram**: the right panel is a simple histogram of the sample, which gives an approximate posterior distribution. From this, we can derive quantities of interest, such as the median (solid line) and a posterior 95% interval (dotted lines).\n",
    "\n",
    "3. **Autocorrelation plot**: the lower left panel shows the pattern of autocorrelation in the sample, with each bar showing the calculated correlation at a given lag. The first step (lag 1) is always highly autocorrelated (by design!) but we hope to see it drop off steeply at bigger lags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymc import Matplot\n",
    "\n",
    "Matplot.plot(mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the estimates for the probabilities of mortality for each hospital. The expected value (population mean) is shown with the dashed line. The empirical estimates (observed deaths divided by the number of surgeries) are shown with red dots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean = 1./(1 + numpy.exp(-mu.stats()['mean']))\n",
    "Matplot.summary_plot(p, vline_pos=mean, x_range=(0,0.17), \n",
    "                     custom_labels=['Hospital %i' % (i+1) for i in range(12)],\n",
    "                     main='Estimated probabilities of death')\n",
    "plt.plot(death_rate, -(numpy.arange(12)+1), 'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Check the Model\n",
    "\n",
    "One way to check our model is to use **posterior predictive checks**. This approach simulates data from the model we have just fit, and compares the simulated data to the observed data. If the model is a decent fit to the data, then it should be plausible that the data could have been sampled from the distribution of simulated data; if the data appears as an outlier, this may be evidence for lack of fit.\n",
    "\n",
    "It is easy to simulate data from a Bayesian model in general, and from PyMC in particular. The only difference between actual (observed) data and simulated (unobserved) data is that we have observed the former, and not the latter. So, we need to copy our likelihood function, remove the `oberved=True` argument, and rename it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "death_pred = Binomial('d_pred', n=surgeries, p=p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add this variable to the model, and run it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "variables.append(death_pred)\n",
    "meta_analysis_check = MCMC(variables)\n",
    "meta_analysis_check.sample(20000, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An informal way of evaluating the posterior predictive checks is to plot them against the data. The `gof_plot` function in PyMC (`gof` stands for \"goodness of fit\") presents a histogram of the simulated values, along with a vertical red dotted line at the location of the observed value.\n",
    "\n",
    "The plot below shows the result for the number of deaths at each hospital."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Matplot.gof_plot(death_pred, deaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatives\n",
    "\n",
    "Though the statistical computing landscape for Bayesian computing is sparse relative to that for classical/frequentist statistics, PyMC is by no means the only tool for fitting Bayesian models. Here are three of the most popular software packages, each differing importantly from PyMC, and from each other, in several ways.\n",
    "\n",
    "* [WinBUGS](http://www.openbugs.net/w/FrontPage) or [JAGS](http://mcmc-jags.sourceforge.net)\n",
    "* [Stan](http://mc-stan.org)\n",
    "* SAS ([PROC MCMC](http://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_mcmc_sect019.htm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "+ Gelman A, Carlin JB, Stern HS, Dunson DB, Vehtari A, Rubin DB. Bayesian Data Analysis, Third Edition. CRC Press; 2013.\n",
    "+ Davidson-Pilon C, et al. [Probabilistic Programming & Bayesian Methods for Hackers](http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Installing Python and PyMC\n",
    "\n",
    "There are several ways of installing Python on your computer, and some of you (particularly those using Mac OS X) may already have a version installed. Irrespective of this, and particularly if you are a new user, I recommend that you install the all-in-one scientific Python distribution called [Anaconda](https://store.continuum.io/cshop/anaconda/). This includes many of the packages required to do scientific Python programming, and makes it easy to install additional ones, including PyMC. \n",
    "\n",
    "#### Installing PyMC\n",
    "\n",
    "Once you have installed Anaconda, open an IPython notebook (perhaps this one!) and type the following in a code cell (don't forget the exclamation point at the start of the line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install pymc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all goes well, you should be able to import PyMC without any error messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are ready to run the tutorial!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        width: 90%;\n",
       "/*        margin-left:auto;*/\n",
       "/*        margin-right:auto;*/\n",
       "    }\n",
       "    ul {\n",
       "        line-height: 145%;\n",
       "        font-size: 90%;\n",
       "    }\n",
       "    li {\n",
       "        margin-bottom: 1em;\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: Helvetica, serif;\n",
       "    }\n",
       "    h4{\n",
       "        margin-top: 12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "    div.text_cell_render{\n",
       "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 145%;\n",
       "        font-size: 150%;\n",
       "        width: 90%;\n",
       "        margin-left:auto;\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
       "    }\n",
       "/*    .prompt{\n",
       "        display: None;\n",
       "    }*/\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 16pt;\n",
       "        color: #4057A1;\n",
       "        font-style: italic;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
